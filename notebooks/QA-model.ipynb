{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Question Answer Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Packages and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 09:59:02.862623: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QA = pd.read_csv('/Users/kellyshreeve/Desktop/Data-Sets/Externship/qa_merged_clean.csv',\n",
    "                    parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 987122 entries, 0 to 987121\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   Unnamed: 0             987122 non-null  int64  \n",
      " 1   id_q                   987122 non-null  int64  \n",
      " 2   owner_user_id_q        973748 non-null  float64\n",
      " 3   creation_date_q        987122 non-null  object \n",
      " 4   score_q                987122 non-null  int64  \n",
      " 5   title                  987122 non-null  object \n",
      " 6   body_q                 987122 non-null  object \n",
      " 7   body_normalized_q      987120 non-null  object \n",
      " 8   title_normalized       987122 non-null  object \n",
      " 9   body_with_sentences_q  987122 non-null  object \n",
      " 10  title_with_sentences   987122 non-null  object \n",
      " 11  creation_year_q        987122 non-null  int64  \n",
      " 12  id_a                   987122 non-null  float64\n",
      " 13  owner_user_id_a        981755 non-null  float64\n",
      " 14  creation_date_a        987122 non-null  object \n",
      " 15  parent_id              987122 non-null  float64\n",
      " 16  score_a                987122 non-null  float64\n",
      " 17  body_a                 987122 non-null  object \n",
      " 18  body_normalized_a      987115 non-null  object \n",
      " 19  body_with_sentences_a  987117 non-null  object \n",
      " 20  creation_year_a        987122 non-null  float64\n",
      " 21  answer_length          987122 non-null  int64  \n",
      " 22  question_length        987122 non-null  int64  \n",
      "dtypes: float64(6), int64(6), object(11)\n",
      "memory usage: 173.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_QA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                   0\n",
      "id_q                         0\n",
      "owner_user_id_q          13374\n",
      "creation_date_q              0\n",
      "score_q                      0\n",
      "title                        0\n",
      "body_q                       0\n",
      "body_normalized_q            2\n",
      "title_normalized             0\n",
      "body_with_sentences_q        0\n",
      "title_with_sentences         0\n",
      "creation_year_q              0\n",
      "id_a                         0\n",
      "owner_user_id_a           5367\n",
      "creation_date_a              0\n",
      "parent_id                    0\n",
      "score_a                      0\n",
      "body_a                       0\n",
      "body_normalized_a            7\n",
      "body_with_sentences_a        5\n",
      "creation_year_a              0\n",
      "answer_length                0\n",
      "question_length              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_QA.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QA=df_QA.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **QA Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data for questions with answers and scores above 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Score Descriptives\n",
      "count    913099.000000\n",
      "mean          3.239337\n",
      "std          22.090040\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           1.000000\n",
      "75%           3.000000\n",
      "max        8384.000000\n",
      "Name: score_a, dtype: float64\n",
      "\n",
      "Question Score Descriptives\n",
      "count    913099.000000\n",
      "mean          7.769584\n",
      "std          65.424836\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           1.000000\n",
      "75%           3.000000\n",
      "max        5524.000000\n",
      "Name: score_q, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Subset for Q & A with positive scores\n",
    "df_QA = df_QA[(df_QA['score_a'] >= 0) & (df_QA['score_q'] >= 0)]\n",
    "\n",
    "print('Answer Score Descriptives')\n",
    "print(df_QA['score_a'].describe())\n",
    "print()\n",
    "print('Question Score Descriptives')\n",
    "print(df_QA['score_q'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All question and answer scores now have a minimum of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 913,099 remaining Q/A pairs where both question and answer have scores > 0 and every question has at least one answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with specific answer as context\n",
    "result: got an answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                              10\n",
       "id_q                                                                   535\n",
       "owner_user_id_q                                                      154.0\n",
       "creation_date_q                                  2008-08-02 18:43:54+00:00\n",
       "score_q                                                                 40\n",
       "title                    Continuous Integration System for a Python Cod...\n",
       "body_q                   <p>I'm starting work on a hobby project with a...\n",
       "body_normalized_q        i'm starting work on a hobby project with a py...\n",
       "title_normalized         continuous integration system for a python cod...\n",
       "body_with_sentences_q    i'm starting work on a hobby project with a py...\n",
       "title_with_sentences     continuous integration system for a python cod...\n",
       "creation_year_q                                                       2008\n",
       "id_a                                                               61746.0\n",
       "owner_user_id_a                                                     6372.0\n",
       "creation_date_a                                  2008-09-15 00:11:21+00:00\n",
       "parent_id                                                            535.0\n",
       "score_a                                                               13.0\n",
       "body_a                   <p>We use both Buildbot and Hudson for Jython ...\n",
       "body_normalized_a        we use both buildbot and hudson for jython dev...\n",
       "body_with_sentences_a    we use both buildbot and hudson for jython dev...\n",
       "creation_year_a                                                     2008.0\n",
       "answer_length                                                          217\n",
       "question_length                                                        109\n",
       "Name: 10, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose row 10\n",
    "display(df_QA.loc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a relevant question for row 10\n",
    "question = 'how to do hobby project?'\n",
    "answer_text = df_QA.loc[10, 'body_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: build it yourself\n",
      "Computation Time: 4.39 seconds\n"
     ]
    }
   ],
   "source": [
    "# BERT QA with row 10\n",
    "start_time = time.time()\n",
    "\n",
    "input_ids = bert_tokenizer.encode(question, answer_text)\n",
    "\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "output = bert_model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "\n",
    "start_index = torch.argmax(output[0][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "end_index = torch.argmax(output[1][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "\n",
    "answer = bert_tokenizer.decode(input_ids[start_index:end_index + 1], skip_special_tokens=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Answer: {answer}')\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find similar questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding for first 10,000 Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QA = df_QA.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Embeddings Shape: (10001, 768)\n",
      "Computation Time: 1181.21 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings for data set questions\n",
    "start_time = time.time()\n",
    "\n",
    "questions = df_QA.loc[0:10000, 'body_with_sentences_q']\n",
    "\n",
    "sent_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "ques_embeddings = sent_model.encode(questions)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Question Embeddings Shape: {ques_embeddings.shape}')\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/kellyshreeve/Desktop/ques_embeddings', 'wb') as file:\n",
    "    pickle.dump(ques_embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/Users/kellyshreeve/desktop/ques_embeddings', 'rb')\n",
    "pickled_embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickled_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find similar questions with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "\n",
      "so when playing with the development i can just set settings.debug to true and if an error occures i can see it nicely formatted with good stack trace and request information. but on kind of production site i'd rather use debug false and show visitors some standard error page with information that i'm working on fixing this bug at this moment br at the same time i'd like to have some way of logging all those information stack trace and request info to a file on my server so i can just output it to my console and watch errors scroll email the log to me every hour or something like this. what logging solutions would you recomend for a django site that would meet those simple requirements i have the application running as fcgi server and i'm using apache web server as frontend although thinking of going to lighttpd .\n",
      "\n",
      "Answer:\n",
      "\n",
      "well when debug false django will automatically mail a full traceback of any error to each person listed in the admins setting which gets you notifications tty much for free. if you'd like more fine grained control you can write and add to your settings a middleware class which defines a method named process_exception which will have access to the exception that was raised http docs.djangoproject.com en dev topics http middleware process exception http docs.djangoproject.com en dev topics http middleware process exception your process_exception method can then perform whatever type of logging you'd like writing to console writing to a file etc. etc. edit though it's a bit less useful you can also listen for the got_request_exception signal which will be sent whenever an exception is encountered during request processing http docs.djangoproject.com en dev ref signals got request exception http docs.djangoproject.com en dev ref signals got request exception this does em not em give you access to the exception object however so the middleware method is much easier to work with.\n",
      "\n",
      "Best Index: 5822\n",
      "\n",
      "Computation Time: 1.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use cosine distance to find similar questions\n",
    "start_time = time.time()\n",
    "\n",
    "new_question = 'What is pandas?'\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "new_question_embeddings = model.encode(new_question)\n",
    "\n",
    "similarity_scores = cosine_similarity([new_question_embeddings],\n",
    "                                       ques_embeddings)\n",
    "\n",
    "best_index = np.argmin(similarity_scores)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "best_question = df_QA.loc[best_index, 'body_with_sentences_q']\n",
    "best_answer = df_QA.loc[best_index, 'body_with_sentences_a']\n",
    "\n",
    "print('Question Posed:')\n",
    "print(new_question)\n",
    "print()\n",
    "print('Question:')\n",
    "print()\n",
    "print(best_question)\n",
    "print()\n",
    "print('Answer:')\n",
    "print()\n",
    "print(best_answer)\n",
    "print()\n",
    "print(f'Best Index: {best_index}')\n",
    "print()\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find answer to question from answer to similar question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "\n",
      "Computation Time: 1.97 seconds\n"
     ]
    }
   ],
   "source": [
    "# Find answer from most relevant question\n",
    "start_time = time.time()\n",
    "\n",
    "new_question = 'What is pandas?'\n",
    "\n",
    "input_ids = bert_tokenizer.encode(new_question, df_QA.loc[5822, 'body_a'])\n",
    "\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "output = bert_model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "\n",
    "start_index = torch.argmax(output[0][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "end_index = torch.argmax(output[1][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "\n",
    "answer = bert_tokenizer.decode(input_ids[start_index:end_index + 1], skip_special_tokens=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Question: {new_question}')\n",
    "print()\n",
    "print(f'Answer: {answer}')\n",
    "print()\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT did not find an answer in the given answer text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that BERT QA works for a more relevant question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: django will automatically mail a full traceback of any error to each person listed in the < code > admins < / code > setting\n",
      "\n",
      "Computation Time: 2.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# Find an answer from a more relevant question\n",
    "start_time = time.time()\n",
    "\n",
    "relevant_question = 'How to use django?'\n",
    "\n",
    "input_ids = bert_tokenizer.encode(relevant_question, df_QA.loc[5822, 'body_a'])\n",
    "\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "output = bert_model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "\n",
    "start_index = torch.argmax(output[0][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "end_index = torch.argmax(output[1][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "\n",
    "answer = bert_tokenizer.decode(input_ids[start_index:end_index + 1], skip_special_tokens=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Question: {relevant_question}')\n",
    "print(f'Answer: {answer}')\n",
    "print()\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not a great answer, BERT did find an answer in the answer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar question function\n",
    "def find_similar_question(question, df, question_column):\n",
    "    new_question_embeddings = model.encode(new_question)\n",
    "\n",
    "    similarity_scores = cosine_similarity([new_question_embeddings],\n",
    "                                        ques_embeddings)\n",
    "\n",
    "    best_index = np.argmin(similarity_scores)\n",
    "\n",
    "    print(f'Posed Question: {question}')\n",
    "    print(f'Most Similar Question: {df.loc[best_index, question_column]})\n",
    "\n",
    "# Try new questions\n",
    "find_similar_question('What is python?', df_QA, 'body_with_sentences_q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings for first 10,000 answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Embeddings Shape: (10001, 768)\n",
      "Computation Time: 1125.99 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings for data set answers\n",
    "start_time = time.time()\n",
    "\n",
    "sentence_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "answers = df_QA.loc[0:10000, 'body_with_sentences_a']\n",
    "\n",
    "answer_embeddings = sentence_model.encode(answers)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Question Embeddings Shape: {ques_embeddings.shape}')\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find answers similar to question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Posed:\n",
      "\n",
      "What is pandas?\n",
      "\n",
      "Question:\n",
      "\n",
      "i recently discovered the notify extension in mercurial which allows me quickly send out emails whenever i push changes but i'm tty sure i'm still missing out on a lot of functionality which could make my life a lot easier. ul li notify extension http www.selenic.com mercurial wiki index.cgi notifyextension rel nofollow http www.selenic.com mercurial wiki index.cgi notifyextension li ul which mercurial hook or combination of interoperating hooks is the most useful for working in a loosely connected team please add links to non standard parts you use and or add the hook or a description how to set it up so others can easily use it.\n",
      "\n",
      "Answer:\n",
      "\n",
      "i really enjoy what i did with my custom hook. i have it post a message to my campfire account campfire is a group based app . it worked out really well. because i had my clients in there and it could show him my progress.\n",
      "\n",
      "Best Index: 1419\n",
      "\n",
      "Computation Time: 1.18 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use cosine distance to find answer similar to question\n",
    "start_time = time.time()\n",
    "\n",
    "new_question = 'What is pandas?'\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "new_question_embeddings = sentence_model.encode(new_question)\n",
    "\n",
    "similarity_scores = cosine_similarity([new_question_embeddings],\n",
    "                                       answer_embeddings)\n",
    "\n",
    "best_index = np.argmin(similarity_scores)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "best_question = df_QA.loc[best_index, 'body_with_sentences_q']\n",
    "best_answer = df_QA.loc[best_index, 'body_with_sentences_a']\n",
    "\n",
    "print('Question Posed:')\n",
    "print()\n",
    "print(new_question)\n",
    "print()\n",
    "print('Question:')\n",
    "print()\n",
    "print(best_question)\n",
    "print()\n",
    "print('Answer:')\n",
    "print()\n",
    "print(best_answer)\n",
    "print()\n",
    "print(f'Best Index: {best_index}')\n",
    "print()\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also not a relevant answer. Difficult to know if it would work better with more embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer():\n",
    "    # Text normaliztion\n",
    "    def normalize_text(text):\n",
    "        text = text.lower()\n",
    "        text = text.replace('<p>', ' ')\n",
    "        text = text.replace('</p>', ' ')\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('<a', ' ')\n",
    "        text = text.replace('</a>', ' ')\n",
    "        text = text.replace('href=', ' ')\n",
    "        text = text.replace('</code', ' ')\n",
    "        text = text.replace('</pre>', ' ')\n",
    "        text = text.replace('<code>', ' ')\n",
    "        text = text.replace('jpeg', ' ')\n",
    "        text = text.replace('jpg', ' ')\n",
    "        text = text.replace('pre', ' ')\n",
    "        text = text.replace('pdf', ' ')\n",
    "        text = re.sub(r\"[^a-zA-z']\", ' ', text)\n",
    "        text = text.split()\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "        return text\n",
    "    \n",
    "    def bert_qa(text):\n",
    "        # ! DEFINE BERT QA FUNCTION\n",
    "        text\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # User input question \n",
    "    question = input('Question:')\n",
    "    \n",
    "    # Normalize Question\n",
    "    question_norm = normalize_text(question)\n",
    "    \n",
    "    # Use bert model to answer question\n",
    "    answer = bert_qa(question_norm)\n",
    "    \n",
    "    # Print answer\n",
    "    print(f'Normalized quesiton: {answer}')\n",
    "    \n",
    "    # User vote\n",
    "    user_vote = 0\n",
    "    \n",
    "    vote = input('Was this question helpful? (y/n)')\n",
    "    \n",
    "    # ! ADD VOTE TO DATAFRAME COLUMN\n",
    "    if vote == 'y':\n",
    "        user_vote += 1\n",
    "    elif vote =='n':\n",
    "        user_vote -= 1\n",
    "    else:\n",
    "        vote = input('Please choose on of these options (y/n):')\n",
    "        \n",
    "    return user_vote"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
