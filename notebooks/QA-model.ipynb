{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Question Answer Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Packages and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 12:06:06.708716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multiple files found in ZIP file. Only one file per ZIP: ['qa_merged_clean.csv', '__MACOSX/._qa_merged_clean.csv']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_QA \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m/Users/kellyshreeve/Desktop/Data-Sets/Externship/qa_merged_clean.zip\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      2\u001b[0m                     compression\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m                     parse_dates\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:793\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mZero files found in ZIP file \u001b[39m\u001b[39m{\u001b[39;00mpath_or_buf\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    792\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 793\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    794\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mMultiple files found in ZIP file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOnly one file per ZIP: \u001b[39m\u001b[39m{\u001b[39;00mzip_names\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             )\n\u001b[1;32m    798\u001b[0m \u001b[39m# TAR Encoding\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[39melif\u001b[39;00m compression \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtar\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Multiple files found in ZIP file. Only one file per ZIP: ['qa_merged_clean.csv', '__MACOSX/._qa_merged_clean.csv']"
     ]
    }
   ],
   "source": [
    "df_QA = pd.read_csv('/Users/kellyshreeve/Desktop/Data-Sets/Externship/qa_merged_clean.zip',\n",
    "                    compression='zip',\n",
    "                    parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 987122 entries, 0 to 987121\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   Unnamed: 0             987122 non-null  int64  \n",
      " 1   id_q                   987122 non-null  int64  \n",
      " 2   owner_user_id_q        973748 non-null  float64\n",
      " 3   creation_date_q        987122 non-null  object \n",
      " 4   score_q                987122 non-null  int64  \n",
      " 5   title                  987122 non-null  object \n",
      " 6   body_q                 987122 non-null  object \n",
      " 7   body_normalized_q      987120 non-null  object \n",
      " 8   title_normalized       987122 non-null  object \n",
      " 9   body_with_sentences_q  987122 non-null  object \n",
      " 10  title_with_sentences   987122 non-null  object \n",
      " 11  creation_year_q        987122 non-null  int64  \n",
      " 12  id_a                   987122 non-null  float64\n",
      " 13  owner_user_id_a        981755 non-null  float64\n",
      " 14  creation_date_a        987122 non-null  object \n",
      " 15  parent_id              987122 non-null  float64\n",
      " 16  score_a                987122 non-null  float64\n",
      " 17  body_a                 987122 non-null  object \n",
      " 18  body_normalized_a      987115 non-null  object \n",
      " 19  body_with_sentences_a  987117 non-null  object \n",
      " 20  creation_year_a        987122 non-null  float64\n",
      " 21  answer_length          987122 non-null  int64  \n",
      " 22  question_length        987122 non-null  int64  \n",
      "dtypes: float64(6), int64(6), object(11)\n",
      "memory usage: 173.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_QA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                   0\n",
      "id_q                         0\n",
      "owner_user_id_q          13374\n",
      "creation_date_q              0\n",
      "score_q                      0\n",
      "title                        0\n",
      "body_q                       0\n",
      "body_normalized_q            2\n",
      "title_normalized             0\n",
      "body_with_sentences_q        0\n",
      "title_with_sentences         0\n",
      "creation_year_q              0\n",
      "id_a                         0\n",
      "owner_user_id_a           5367\n",
      "creation_date_a              0\n",
      "parent_id                    0\n",
      "score_a                      0\n",
      "body_a                       0\n",
      "body_normalized_a            7\n",
      "body_with_sentences_a        5\n",
      "creation_year_a              0\n",
      "answer_length                0\n",
      "question_length              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_QA.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QA=df_QA.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **QA Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data for questions with answers and scores above 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Score Descriptives\n",
      "count    913099.000000\n",
      "mean          3.239337\n",
      "std          22.090040\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           1.000000\n",
      "75%           3.000000\n",
      "max        8384.000000\n",
      "Name: score_a, dtype: float64\n",
      "\n",
      "Question Score Descriptives\n",
      "count    913099.000000\n",
      "mean          7.769584\n",
      "std          65.424836\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           1.000000\n",
      "75%           3.000000\n",
      "max        5524.000000\n",
      "Name: score_q, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Subset for Q & A with positive scores\n",
    "df_QA = df_QA[(df_QA['score_a'] >= 0) & (df_QA['score_q'] >= 0)]\n",
    "\n",
    "print('Answer Score Descriptives')\n",
    "print(df_QA['score_a'].describe())\n",
    "print()\n",
    "print('Question Score Descriptives')\n",
    "print(df_QA['score_q'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All question and answer scores now have a minimum of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 913,099 remaining Q/A pairs where both question and answer have scores > 0 and every question has at least one answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with specific answer as context\n",
    "result: got an answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                              10\n",
       "id_q                                                                   535\n",
       "owner_user_id_q                                                      154.0\n",
       "creation_date_q                                  2008-08-02 18:43:54+00:00\n",
       "score_q                                                                 40\n",
       "title                    Continuous Integration System for a Python Cod...\n",
       "body_q                   <p>I'm starting work on a hobby project with a...\n",
       "body_normalized_q        i'm starting work on a hobby project with a py...\n",
       "title_normalized         continuous integration system for a python cod...\n",
       "body_with_sentences_q    i'm starting work on a hobby project with a py...\n",
       "title_with_sentences     continuous integration system for a python cod...\n",
       "creation_year_q                                                       2008\n",
       "id_a                                                               61746.0\n",
       "owner_user_id_a                                                     6372.0\n",
       "creation_date_a                                  2008-09-15 00:11:21+00:00\n",
       "parent_id                                                            535.0\n",
       "score_a                                                               13.0\n",
       "body_a                   <p>We use both Buildbot and Hudson for Jython ...\n",
       "body_normalized_a        we use both buildbot and hudson for jython dev...\n",
       "body_with_sentences_a    we use both buildbot and hudson for jython dev...\n",
       "creation_year_a                                                     2008.0\n",
       "answer_length                                                          217\n",
       "question_length                                                        109\n",
       "Name: 10, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose row 10\n",
    "display(df_QA.loc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a relevant question for row 10\n",
    "question = 'how to do hobby project?'\n",
    "answer_text = df_QA.loc[10, 'body_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: build it yourself\n",
      "Computation Time: 4.39 seconds\n"
     ]
    }
   ],
   "source": [
    "# BERT QA with row 10\n",
    "start_time = time.time()\n",
    "\n",
    "input_ids = bert_tokenizer.encode(question, answer_text)\n",
    "\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "output = bert_model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "\n",
    "start_index = torch.argmax(output[0][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "end_index = torch.argmax(output[1][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "\n",
    "answer = bert_tokenizer.decode(input_ids[start_index:end_index + 1], skip_special_tokens=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Answer: {answer}')\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find similar questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding for first 10,000 Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m questions \u001b[39m=\u001b[39m df_QA\u001b[39m.\u001b[39mloc[\u001b[39m0\u001b[39m:\u001b[39m10000\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbody_with_sentences_q\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mbert-base-nli-mean-tokens\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m ques_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(questions)\n\u001b[1;32m     10\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m computation_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pytorch_utils.py:240\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get embeddings for data set questions\n",
    "start_time = time.time()\n",
    "\n",
    "questions = df_QA.loc[0:10000, 'body_with_sentences_q']\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "ques_embeddings = model.encode(questions)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Question Embeddings Shape: {ques_embeddings.shape}')\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find similar questions with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "\n",
      "so when playing with the development i can just set settings.debug to true and if an error occures i can see it nicely formatted with good stack trace and request information. but on kind of production site i'd rather use debug false and show visitors some standard error page with information that i'm working on fixing this bug at this moment br at the same time i'd like to have some way of logging all those information stack trace and request info to a file on my server so i can just output it to my console and watch errors scroll email the log to me every hour or something like this. what logging solutions would you recomend for a django site that would meet those simple requirements i have the application running as fcgi server and i'm using apache web server as frontend although thinking of going to lighttpd .\n",
      "\n",
      "Answer:\n",
      "\n",
      "well when debug false django will automatically mail a full traceback of any error to each person listed in the admins setting which gets you notifications tty much for free. if you'd like more fine grained control you can write and add to your settings a middleware class which defines a method named process_exception which will have access to the exception that was raised http docs.djangoproject.com en dev topics http middleware process exception http docs.djangoproject.com en dev topics http middleware process exception your process_exception method can then perform whatever type of logging you'd like writing to console writing to a file etc. etc. edit though it's a bit less useful you can also listen for the got_request_exception signal which will be sent whenever an exception is encountered during request processing http docs.djangoproject.com en dev ref signals got request exception http docs.djangoproject.com en dev ref signals got request exception this does em not em give you access to the exception object however so the middleware method is much easier to work with.\n",
      "\n",
      "Best Index: 5822\n",
      "\n",
      "Computation Time: 1.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use cosine distance to find similar questions\n",
    "start_time = time.time()\n",
    "\n",
    "new_question = 'What is pandas?'\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "new_question_embeddings = model.encode(new_question)\n",
    "\n",
    "similarity_scores = cosine_similarity([new_question_embeddings],\n",
    "                                       ques_embeddings)\n",
    "\n",
    "best_index = np.argmin(similarity_scores)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "best_question = df_QA.loc[best_index, 'body_with_sentences_q']\n",
    "best_answer = df_QA.loc[best_index, 'body_with_sentences_a']\n",
    "\n",
    "print('Question Posed:')\n",
    "print(new_question)\n",
    "print()\n",
    "print('Question:')\n",
    "print()\n",
    "print(best_question)\n",
    "print()\n",
    "print('Answer:')\n",
    "print()\n",
    "print(best_answer)\n",
    "print()\n",
    "print(f'Best Index: {best_index}')\n",
    "print()\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find answer to question from answer to similar question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "\n",
      "Computation Time: 1.97 seconds\n"
     ]
    }
   ],
   "source": [
    "# Find answer from most relevant question\n",
    "start_time = time.time()\n",
    "\n",
    "new_question = 'What is pandas?'\n",
    "\n",
    "input_ids = bert_tokenizer.encode(new_question, df_QA.loc[5822, 'body_a'])\n",
    "\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "output = bert_model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "\n",
    "start_index = torch.argmax(output[0][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "end_index = torch.argmax(output[1][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "\n",
    "answer = bert_tokenizer.decode(input_ids[start_index:end_index + 1], skip_special_tokens=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Question: {new_question}')\n",
    "print()\n",
    "print(f'Answer: {answer}')\n",
    "print()\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT did not find an answer in the given answer text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that BERT QA works for a more relevant question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: django will automatically mail a full traceback of any error to each person listed in the < code > admins < / code > setting\n",
      "\n",
      "Computation Time: 2.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# Find an answer from a more relevant question\n",
    "start_time = time.time()\n",
    "\n",
    "relevant_question = 'How to use django?'\n",
    "\n",
    "input_ids = bert_tokenizer.encode(relevant_question, df_QA.loc[5822, 'body_a'])\n",
    "\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "output = bert_model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "\n",
    "start_index = torch.argmax(output[0][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "end_index = torch.argmax(output[1][0, :len(input_ids) - input_ids.index(bert_tokenizer.sep_token_id)])\n",
    "\n",
    "answer = bert_tokenizer.decode(input_ids[start_index:end_index + 1], skip_special_tokens=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Question: {relevant_question}')\n",
    "print(f'Answer: {answer}')\n",
    "print()\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not a great answer, BERT did find an answer in the answer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar question function\n",
    "def find_similar_question(question, df, question_column):\n",
    "    new_question_embeddings = model.encode(new_question)\n",
    "\n",
    "    similarity_scores = cosine_similarity([new_question_embeddings],\n",
    "                                        ques_embeddings)\n",
    "\n",
    "    best_index = np.argmin(similarity_scores)\n",
    "\n",
    "    print(f'Posed Question: {question}')\n",
    "    print(f'Most Similar Question: {df.loc[best_index, question_column]})\n",
    "\n",
    "# Try new questions\n",
    "find_similar_question('What is python?', df_QA, 'body_with_sentences_q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings for first 10,000 answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Embeddings Shape: (10001, 768)\n",
      "Computation Time: 1125.99 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings for data set answers\n",
    "start_time = time.time()\n",
    "\n",
    "sentence_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "answers = df_QA.loc[0:10000, 'body_with_sentences_a']\n",
    "\n",
    "answer_embeddings = sentence_model.encode(answers)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "print(f'Question Embeddings Shape: {ques_embeddings.shape}')\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find answers similar to question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Posed:\n",
      "\n",
      "What is pandas?\n",
      "\n",
      "Question:\n",
      "\n",
      "i recently discovered the notify extension in mercurial which allows me quickly send out emails whenever i push changes but i'm tty sure i'm still missing out on a lot of functionality which could make my life a lot easier. ul li notify extension http www.selenic.com mercurial wiki index.cgi notifyextension rel nofollow http www.selenic.com mercurial wiki index.cgi notifyextension li ul which mercurial hook or combination of interoperating hooks is the most useful for working in a loosely connected team please add links to non standard parts you use and or add the hook or a description how to set it up so others can easily use it.\n",
      "\n",
      "Answer:\n",
      "\n",
      "i really enjoy what i did with my custom hook. i have it post a message to my campfire account campfire is a group based app . it worked out really well. because i had my clients in there and it could show him my progress.\n",
      "\n",
      "Best Index: 1419\n",
      "\n",
      "Computation Time: 1.18 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use cosine distance to find answer similar to question\n",
    "start_time = time.time()\n",
    "\n",
    "new_question = 'What is pandas?'\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "new_question_embeddings = sentence_model.encode(new_question)\n",
    "\n",
    "similarity_scores = cosine_similarity([new_question_embeddings],\n",
    "                                       answer_embeddings)\n",
    "\n",
    "best_index = np.argmin(similarity_scores)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "computation_time = end_time - start_time\n",
    "\n",
    "best_question = df_QA.loc[best_index, 'body_with_sentences_q']\n",
    "best_answer = df_QA.loc[best_index, 'body_with_sentences_a']\n",
    "\n",
    "print('Question Posed:')\n",
    "print()\n",
    "print(new_question)\n",
    "print()\n",
    "print('Question:')\n",
    "print()\n",
    "print(best_question)\n",
    "print()\n",
    "print('Answer:')\n",
    "print()\n",
    "print(best_answer)\n",
    "print()\n",
    "print(f'Best Index: {best_index}')\n",
    "print()\n",
    "print(f'Computation Time: {computation_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also not a relevant answer. Difficult to know if it would work better with more embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer():\n",
    "    # Text normaliztion\n",
    "    def normalize_text(text):\n",
    "        text = text.lower()\n",
    "        text = text.replace('<p>', ' ')\n",
    "        text = text.replace('</p>', ' ')\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('<a', ' ')\n",
    "        text = text.replace('</a>', ' ')\n",
    "        text = text.replace('href=', ' ')\n",
    "        text = text.replace('</code', ' ')\n",
    "        text = text.replace('</pre>', ' ')\n",
    "        text = text.replace('<code>', ' ')\n",
    "        text = text.replace('jpeg', ' ')\n",
    "        text = text.replace('jpg', ' ')\n",
    "        text = text.replace('pre', ' ')\n",
    "        text = text.replace('pdf', ' ')\n",
    "        text = re.sub(r\"[^a-zA-z']\", ' ', text)\n",
    "        text = text.split()\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "        return text\n",
    "    \n",
    "    def bert_qa(text):\n",
    "        # ! DEFINE BERT QA FUNCTION\n",
    "        text\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # User input question \n",
    "    question = input('Question:')\n",
    "    \n",
    "    # Normalize Question\n",
    "    question_norm = normalize_text(question)\n",
    "    \n",
    "    # Use bert model to answer question\n",
    "    answer = bert_qa(question_norm)\n",
    "    \n",
    "    # Print answer\n",
    "    print(f'Normalized quesiton: {answer}')\n",
    "    \n",
    "    # User vote\n",
    "    user_vote = 0\n",
    "    \n",
    "    vote = input('Was this question helpful? (y/n)')\n",
    "    \n",
    "    # ! ADD VOTE TO DATAFRAME COLUMN\n",
    "    if vote == 'y':\n",
    "        user_vote += 1\n",
    "    elif vote =='n':\n",
    "        user_vote -= 1\n",
    "    else:\n",
    "        vote = input('Please choose on of these options (y/n):')\n",
    "        \n",
    "    return user_vote"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
